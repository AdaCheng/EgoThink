{"name": "pair-v2", "type": "pairwise", "system_prompt": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.", "prompt_template": "[User Question]\n{question}\n\n[The Start of Assistant A's Answer]\n{answer_a}\n[The End of Assistant A's Answer]\n\n[The Start of Assistant B's Answer]\n{answer_b}\n[The End of Assistant B's Answer]", "description": "Prompt for general questions", "category": "general", "output_format": "[[A]]"}
{"name": "pair-v2-multi-turn", "type": "pairwise", "system_prompt": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.", "prompt_template": "<|The Start of Assistant A's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant A:\n{answer_a_1}\n\n### User:\n{question_2}\n\n### Assistant A:\n{answer_a_2}\n\n<|The End of Assistant A's Conversation with User|>\n\n\n<|The Start of Assistant B's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant B:\n{answer_b_1}\n\n### User:\n{question_2}\n\n### Assistant B:\n{answer_b_2}\n\n<|The End of Assistant B's Conversation with User|>", "description": "Prompt for multi-turn general questions", "category": "general", "output_format": "[[A]]"}
{"name": "pair-math-v1", "type": "pairwise", "system_prompt": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A's answer, and assistant B's answer. Your job is to evaluate which assistant's answer is better. Begin your evaluation by comparing both assistants' answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.", "prompt_template": "[User Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant A's Answer]\n{answer_a}\n[The End of Assistant A's Answer]\n\n[The Start of Assistant B's Answer]\n{answer_b}\n[The End of Assistant B's Answer]", "description": "Prompt for math questions", "category": "math", "output_format": "[[A]]"}
{"name": "pair-math-v1-multi-turn", "type": "pairwise", "system_prompt": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. Your evaluation should consider correctness and helpfulness. You will be given reference answers, the assistant A's answers, the assistant B's answers. Your job is to determine which assistant provides correct and helpful answers to the second user question. Begin your evaluation by comparing both assistants' answers with the reference answers. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.", "prompt_template": "<|The Start of Reference Answer|>\n\n### User:\n{question_1}\n\n### Reference answer:\n{ref_answer_1}\n\n### User:\n{question_2}\n\n### Reference answer:\n{ref_answer_2}\n\n<|The End of Reference Answer|>\n\n\n<|The Start of Assistant A's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant A:\n{answer_a_1}\n\n### User:\n{question_2}\n\n### Assistant A:\n{answer_a_2}\n\n<|The End of Assistant A's Conversation with User|>\n\n\n<|The Start of Assistant B's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant B:\n{answer_b_1}\n\n### User:\n{question_2}\n\n### Assistant B:\n{answer_b_2}\n\n<|The End of Assistant B's Conversation with User|>", "description": "Prompt for multi-turn general questions", "category": "general", "output_format": "[[A]]"}
{"name": "single-v1", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "[Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n\n[Question]\n{question}\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]", "description": "Prompt for general questions", "category": "general", "output_format": "[[rating]]"}
{"name": "single-math-v1", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "[Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]", "description": "Prompt for general questions", "category": "math", "output_format": "[[rating]]"}
{"name": "single-math-v2", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "[Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. The assistant has access to an image alongwith questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answer. If the assistant's answer is not exactly same as or similar to the answer, then he must be wrong.  Be as objective as possible. Discourage uninformative answers. Also, equally treat short and long answers and focus on the correctness of answers.  After providing your explanation, you must rate the response with either 0, 0.5 or 1 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[0.5]]\".\n\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]", "description": "Prompt for general questions", "category": "math", "output_format": "[[rating]]"}
{"name": "single-v1-multi-turn", "type": "single", "system_prompt": "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on the assistant's answer to the second user question. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n\n", "prompt_template": "<|The Start of Assistant A's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant A:\n{answer_1}\n\n### User:\n{question_2}\n\n### Assistant A:\n{answer_2}\n\n<|The End of Assistant A's Conversation with User|>", "description": "Prompt for general questions", "category": "general", "output_format": "[[rating]]"}
{"name": "single-math-v1-multi-turn", "type": "single", "system_prompt": "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. You evaluation should focus on the assistant's answer to the second question. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n\n", "prompt_template": "<|The Start of Reference Answer|>\n\n### User:\n{question_1}\n\n### Reference answer:\n{ref_answer_1}\n\n### User:\n{question_2}\n\n### Reference answer:\n{ref_answer_2}\n\n<|The End of Reference Answer|>\n\n\n<|The Start of Assistant A's Conversation with User|>\n\n### User:\n{question_1}\n\n### Assistant A:\n{answer_1}\n\n### User:\n{question_2}\n\n### Assistant A:\n{answer_2}\n\n<|The End of Assistant A's Conversation with User|>", "description": "Prompt for general questions", "category": "math", "output_format": "[[rating]]"}
{"name": "single-rm-feedback-v1", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "[Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given three reference answers and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answers. Identify and correct any mistakes. The assistant has access to an image alongwith questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answers. If the assistant's answer is not exactly same as or similar to all reference answers, then he must be wrong. If the assistant's answer is exactly same as or similar to any one reference answer, then it is correct. Be as objective as possible. Discourage uninformative answers. Also, equally treat short and long answers and focus on the correctness of answers.  After providing your explanation, you must rate the response with either 0, 0.5 or 1 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[0.5]]\".\n\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]", "description": "Prompt for general questions", "category": "rm_feedback", "output_format": "[[rating]]"}
{"name": "hp-h2m-v1", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "\n[Instruction]\nPlease act as an impatial judge and evaluate the quality of the response provided by a label and a prediction to the prompt displayed below. Begin your evaluation by comparing the responses and provide a short explanation . Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favour certain names of the assistants. After providing your explanation, you will You will output a rating of 1 for prediction that is the exact idea as the label, 0.5 for outputs that partially resembles the label, and 0.0 for ouputs that does not resemble label. After, you provide your explanation, output your final verdict by strictly following this format: \"[[1.0]]\" if the prediction closely resembles the label, \"[[0.5]]\" if prediction partially resembles the label, and \"[[0.0]]\" if the prediction does not resemble the label at all.\nGiven an label and a prediction, you are tasked to determine how closely the prediction resembles the label. You will output a rating of 1 for prediction that is the exact idea as the label, 0.5 for outputs that partially resembles the label, and 0.0 for ouputs that does not resemble label.\nOutput your marking strictly in the following format \"[[1]]\", \"[[0.5]]\", \"[[0.0]]\"\n\nGuidelines\n- The label and prediction both describes a mid level step towards completing a overarching highlevel goals, you must consider if these two mid level steps are similar in nature.\n- In the prediction, if the either action or the item that the mid level step is described to be executed with this can be considered as a 0.5 as long as the intention is the same as the label.\n- If the prediction fails to specify what object it is performing the action on but succeed in predicting the action when compared to the label, we give it a rating of 0.5.\n- If the predicted action is different but shares very similar intent with the label, we give it a rating of 0.5.\n- If the predicted mid level step is completely different from the label or the description in the prediction seems impossible to complete, a rating of 0 maybe assigned.\n- When providing a rating, you  must also consider the [question] which provided information on what the prediction is for. Consider the question and factor in whether it covers the [question] provided. \n\n[Start of Reference Answer]\nreference answer 1 - prepares dough in dough mixer\nprediction 1 - From the supplementary material provided, the nextThe most likely next mid-level step is to add the ingredients into the dough mixer.\nrating 1 - \"[[1.0]]\"\n\nreference answer 2-  pours water in oven furnace\nprediction 2 - Fill the container with water\nrating 2 - \"[[0.5]]\"\n\nreference answer 3 - The next mid-level step is to open the oven to check the condition of the food inside.\nprediction 3 - adjusts baking pans in oven with long rod\nrating 3 - \"[[0.5]]\"\n\nreference answer 4 - cut dough with dough cutter\nprediction 4 - Preheat the oven to the required temperature for baking the goods.\nrating 4 - \"[[0.0]]\"\n[End of Reference Answer]\n\n[Start of Question]{question}[End of Question]reference answer:\n{ref_answer_1}\nprediction:\n{answer}\n\nGiven the prediction and label above, provide your rating in strictly in the following format.\n\"[[xxx]]\" (xxx can only \"[[0.0]]\", \"[[0.5]]\", or \"[[1.0]]\")\n","description": "Prompt for general questions", "category": "hp_h2m","output_format": "[[rating]]"}
{"name": "hp-m2l-v1", "type": "single", "system_prompt": "You are a helpful assistant.", "prompt_template": "\n[Instruction]\nPlease act as an impatial judge and evaluate the quality of the response provided by a label and a prediction to the prompt displayed below. Begin your evaluation by comparing the responses and provide a short explanation.\nAvoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favour certain names of the assistants. \nAfter providing your explanation, you will You will output a rating between 0-10.\n You must strictly output your verdict in the following format: \"[[xxx]]\" where xxx denotes a rating ranging from 0-10. A hierarchical planning task can be encapsulated by an overarching, high-level goal; this can be further broken down into mid-level sub-goals that separate the hierarchical task into low-level, executable steps. \nA low-level sequence is characterized as a sequence sequence of function that are called. These functions correspond to some real atomic action an individual may perform (e.g., put(apple, table)) refers to the action of putting the apple on the table).\nFor the mid-level goal, you are presented with a reference sequence of low-level step and a predicted sequence of low-level step. You are task is to determine how closely the predicted sequence resembles for the reference sequence which both are both trying to complete the mid level step.\nYour must provide a mark on a scale of 1 to 4 on how closely these two low-level goals resemble each other following the guidelines below. When making marking, you must be impartial to the order in which the two sequences of low-level functions appear. \nWhen providing your response, please strictly follow the format of “[[rating]]”. Do not output any explanation and reasoning in the process.\n\nGuidelines:\n- A very high rating can be given for little to no deviations between the reference answer and the predicted answer. The order in which the low-level steps are carried out is correct and all necessary low-level actions are in place.\n- A high rating can be given if there exists minor deviations between the arguments or the names of the low-level functions. The ordering in which the two sequences are relatively similar and the predicted order is reasonable enough. The function calls may have minor difference, but predicted sequence achieves the outcome of the reference sequence for the most part.\n- Medium rating can be given there exists an obvious deviation between the arguments or the names of the low-level function, and there is a noticeable difference in the sequence in which low-level actions are carried out; or the function calls have noticeable difference, but the predicted sequence partially achieves the outcomes of the reference sequences.  Assign the rating with reference to the varying degree in which the predicted sequence achive the goal.\n- A low rating shoul be given if there are little to no similarities between the arguments or the names of the two sequences of low-level functions, and the ordering of the sequence is completely different or if the predicted sequence does not achieve the outcome of the reference seqeuence in any way, shape, or form. \n- When assigning rating the sequence length can be a factor to consider however this does not mean short sequence are lesser than the longer ones. Repetition of functions that appear unnecessary should be penalized. A sequence of low level action can be extremely long but that does not mean it capture the essence of the reference answer should be penalize. Do not consider quantity over quality.\n- When rating the prediction, also consider the question provided since it explicitly states the goal/task one is trying to complete. Factor in whether the task described in the question can be completed with the sequence of low-level function provided. \n\n[Start of Question]\n{question}\n\n[End of Question]\n\n[Start of Reference Answer]\n{ref_answer_1}\n[End of Reference Answer]\n[Start of predicted answer]\n{answer}\n[End of predicted answer]\n\nGiven the two sequences above, while adhering to the guidelines, please provide your response strictly in the following format:\n“[[xxx]]” (xxx can be 1.0-10.0)\n","description": "Prompt for general questions","category": "hp_m2l", "output_format": "[[rating]]"}